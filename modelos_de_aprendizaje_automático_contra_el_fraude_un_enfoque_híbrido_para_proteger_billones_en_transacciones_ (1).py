# -*- coding: utf-8 -*-
"""MODELOS DE APRENDIZAJE AUTOMÁTICO CONTRA EL FRAUDE: UN ENFOQUE HÍBRIDO PARA PROTEGER BILLONES EN TRANSACCIONES.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-UdYJioj9mCb2fUXx9vcbC3TO8OSK4IW

MODELOS DE APRENDIZAJE AUTOMÁTICO CONTRA EL FRAUDE: UN ENFOQUE HÍBRIDO PARA PROTEGER BILLONES EN TRANSACCIONES.
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Carga de datos (reemplaza 'data.csv' con tu propio archivo de datos)
df = pd.read_csv('/content/creditcard.csv')

# Eliminar filas con valores faltantes
df.dropna(inplace=True)
#seleccion de la data
X=df.iloc[:, 1:30]
y=df.iloc[:,30]

import pandas as pd

# Carga de datos desde un archivo CSV
df = pd.read_csv('/content/creditcard.csv')

# Mostrar los conteos de ceros y unos en la columna 'y'
conteo_y = df['Class'].value_counts()

print("Conteo de ceros y unos en la columna 'y':")
print(conteo_y)

# Calcular el total de observaciones en la columna 'y'
total_y = df['Class'].count()

print("Total de observaciones en la columna 'y':", total_y)

# Calcular el porcentaje de ceros y unos en la columna 'y'
porcentaje_y = df['Class'].value_counts(normalize=True) * 100

print("Porcentaje de ceros y unos en la columna 'y':")
print(porcentaje_y)

"""Etapa de separacion de la data para entrenamiento y prueba"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""Normalización con MInMAxScaler"""

from sklearn.preprocessing import MinMaxScaler
normalizar = MinMaxScaler()
X_train = normalizar.fit_transform(X_train)
X_test = normalizar.transform(X_test)
import pandas as pd
y_test = pd.DataFrame(y_test)
y_test = normalizar.fit_transform(y_test)
print(y_test)
y_train.shape
y_test.shape
import numpy as np
print(type(y_train))
print(type(y_test))
y_train = np.array(y_train).reshape(-1, 1)
y_test = np.array(y_test).reshape(-1, 1)
y_train = normalizar.fit_transform(y_train)
y_test = normalizar.transform(y_test)
# Estimación de la matriz de covarianza robusta
robust_cov = MinCovDet().fit(X_train)

"""Muestra el valor optimo de componentes principales mediante el metodo del codo"""

import matplotlib.pyplot as plt

# Aplicar PCA sin especificar el número de componentes
pca = PCA()
pca.fit(X_scaled)

# Variación explicada acumulativa
explained_variance_ratio_cumulative = np.cumsum(pca.explained_variance_ratio_)

# Graficar la variación explicada acumulativa
plt.plot(explained_variance_ratio_cumulative, marker='o', linestyle='--')
plt.xlabel('Número de componentes')
plt.ylabel('Variación explicada acumulativa')
plt.title('Variación explicada acumulativa por número de componentes')
plt.grid(True)
plt.show()

# Calcular los componentes principales utilizando la matriz de covarianza robusta
pca = PCA(n_components=2)
pca.fit(X_scaled)
X_pca = pca.transform(X_scaled)

"""Gráfico de scree plot: Muestra la varianza explicada por cada componente principal."""

# Mapa de puntos de los datos en el espacio de los componentes principales
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c='b', alpha=0.6)
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('Mapa de puntos en el espacio de los componentes principales')
plt.grid(True)
plt.show()

"""Gráfico de carga de factores: Indica la relación entre las variables originales y los componentes principales."""

# Gráfico de scree plot
plt.figure(figsize=(8, 5))
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, color=['blue', 'green'])
plt.xlabel('Componente Principal')
plt.ylabel('Varianza explicada')
plt.title('Gráfico de scree plot')
plt.show()

"""Mapa de puntos de los datos en el espacio de los componentes principales: Muestra cómo se distribuyen los datos en el espacio de los componentes principales."""

# Gráfico de carga de factores
plt.figure(figsize=(8, 5))
plt.matshow(pca.components_, cmap='viridis')
plt.yticks([0, 1], ['Componente 1', 'Componente 2'], fontsize=10)
plt.colorbar()
plt.xticks(range(len(X[0])), ['Feature 1', 'Feature 2'], rotation=45, ha='left')
plt.xlabel('Variables originales')
plt.title('Gráfico de carga de factores')
plt.show()

"""Detección de outliers y valores atípicos: Identifica los outliers y valores atípicos en los datos, y la visualización de la estructura de los datos: Muestra una dispersión de los datos con outliers resaltados."""

# Detección de outliers y valores atípicos
distances = np.sum((X_scaled - robust_cov.location_) ** 2, axis=1)
threshold = np.percentile(distances, 95) # Establecer un umbral del 95%
outliers = np.where(distances > threshold)[0]
print("Índices de outliers:", outliers)

# Visualización de la estructura de los datos
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c='b', alpha=0.6, label='Datos')
plt.scatter(X_scaled[outliers, 0], X_scaled[outliers, 1], c='r', marker='o', label='Outliers')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Visualización de la estructura de los datos')
plt.legend()
plt.grid(True)
plt.show()

"""para obtener un resultado de la precicion el ACP se combina con un metodo ejemplo KNN"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

# Generar datos de ejemplo
import pandas as pd

# Carga de datos (reemplaza 'data.csv' con tu propio archivo de datos)
df = pd.read_csv('/content/creditcard.csv')

# Eliminar filas con valores faltantes
df.dropna(inplace=True)

X=df.iloc[:, 1:30]
y=df.iloc[:,30]



# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA con estimación de covarianza robusta
pca = PCA(n_components=10, svd_solver='randomized', whiten=True)
pca.fit(X_scaled)
X_pca = pca.transform(X_scaled)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Definir el clasificador KNN
k = 5  # Número de vecinos
knn = KNeighborsClassifier(n_neighbors=k)

# Entrenar el clasificador
knn.fit(X_train, y_train)

# Predecir en el conjunto de prueba
y_pred = knn.predict(X_test)

# Calcular la precisión
accuracy = accuracy_score(y_test, y_pred)
print("Precisión:", accuracy)

"""Validación crusada para el metodo KNN"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

# Generar datos de ejemplo
import pandas as pd

# Carga de datos (reemplaza 'data.csv' con tu propio archivo de datos)
df = pd.read_csv('/content/creditcard.csv')

# Eliminar filas con valores faltantes
df.dropna(inplace=True)

X=df.iloc[:, 1:30]
y=df.iloc[:,30]



# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA con estimación de covarianza robusta
pca = PCA(n_components=10, svd_solver='randomized', whiten=True)
pca.fit(X_scaled)
X_pca = pca.transform(X_scaled)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Definir el clasificador KNN
k = 5  # Número de vecinos
knn = KNeighborsClassifier(n_neighbors=k)

# Entrenar el clasificador
knn.fit(X_train, y_train)

# Predecir en el conjunto de prueba
y_pred = knn.predict(X_test)

# Calcular la precisión
accuracy = accuracy_score(y_test, y_pred)
print("Precisión:", accuracy)

from sklearn.model_selection import cross_val_score
#k vecino que mejore el modelo
Knn=KNeighborsClassifier(n_neighbors=5)
puntajes=cross_val_score(Knn,X,y,cv=5,scoring='accuracy')
print(puntajes)
print(puntajes.mean())

# Curva de aprendizaje
train_sizes, train_scores, test_scores = learning_curve(knn, X_train, y_train, cv=5)
train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_scores_mean, label='Training score')
plt.plot(train_sizes, test_scores_mean, label='Cross-validation score')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.title('Learning Curve')
plt.legend()
plt.grid(True)
plt.show()

# Matriz de confusión
y_pred = knn.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Tráfico de errores en función de K
error_rates = []
k_values = range(1, 21)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    error_rates.append(1 - accuracy_score(y_test, y_pred))

plt.figure(figsize=(8, 6))
plt.plot(k_values, error_rates, marker='o', linestyle='-', color='b')
plt.xlabel('K')
plt.ylabel('Error Rate')
plt.title('Error Rate vs K')
plt.xticks(range(1, 21))
plt.grid(True)
plt.show()


# Gráfico de importancia de las características
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X_train, y_train)

plt.figure(figsize=(10, 6))
plt.bar(range(len(selector.scores_)), selector.scores_, color='skyblue')
plt.xticks(range(len(selector.scores_)), [f'Feature {i+1}' for i in range(len(selector.scores_))])
plt.xlabel('Feature')
plt.ylabel('Score')
plt.title('Feature Importance Scores')
plt.grid(True)
plt.show()

"""Metodo de ACP con NN"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

# Generar datos de ejemplo
import pandas as pd

# Carga de datos (reemplaza 'data.csv' con tu propio archivo de datos)
df = pd.read_csv('/content/creditcard.csv')

# Eliminar filas con valores faltantes
df.dropna(inplace=True)

X=df.iloc[:, 1:30]
y=df.iloc[:,30]



# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA con estimación de covarianza robusta
pca = PCA(n_components=10, svd_solver='randomized', whiten=True)
pca.fit(X_scaled)
X_pca = pca.transform(X_scaled)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Crear una instancia del clasificador MLP
modeloMlP = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)

# Entrenar el modelo con los datos de entrenamiento
modeloMlP.fit(X_train, y_train)

# Predecir las etiquetas de los datos de prueba
y_predichas = modeloMlP.predict(X_test)

# Calcular la precisión del modelo
accuracy = accuracy_score(y_test, y_predichas)
print("Precisión del modelo MLP:", accuracy)

"""validación crusada"""

from sklearn.model_selection import cross_val_score
#k vecino que mejore el modelo

puntajes=cross_val_score(modeloMlP,X,y,cv=5,scoring='accuracy')
print(puntajes)
print(puntajes.mean())

!pip install scikit-learn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import learning_curve

# Escalar los datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Entrenar el clasificador MLP
modeloMlP.fit(X_train_scaled, y_train)

# Curva de aprendizaje
train_sizes = np.linspace(0.1, 1.0, 10)
train_sizes, train_scores, test_scores = learning_curve(modeloMlP, X_train_scaled, y_train, train_sizes=train_sizes, cv=5)
train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_scores_mean, label='Training score')
plt.plot(train_sizes, test_scores_mean, label='Cross-validation score')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.title('Learning Curve')
plt.legend()
plt.grid(True)
plt.show()

# Matriz de confusión
y_pred = mlp.predict(X_test_scaled)
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()
# Gráfico de ROC y AUC
y_proba = mlp.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc_score = roc_auc_score(y_test, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

# Gráfico de importancia de las características
feature_importance = mlp.coefs_[0]
sns.set(style='whitegrid')
plt.figure(figsize=(10, 6))
sns.barplot(x=np.arange(1, len(feature_importance)+1), y=np.mean(feature_importance, axis=1), palette='viridis')
plt.xlabel('Feature Index')
plt.ylabel('Mean Weight')
plt.title('Feature Importance')
plt.show()

# Visualización de las capas ocultas
plt.figure(figsize=(12, 8))
for i, coef in enumerate(mlp.coefs_):
    ax = plt.subplot(1, len(mlp.coefs_), i + 1)
    sns.heatmap(coef, cmap='viridis', ax=ax)
    ax.set_title(f'Capa Oculta {i+1}')
plt.show()

"""Metodo de ACP con SVM"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

# Generar datos de ejemplo
import pandas as pd

# Carga de datos (reemplaza 'data.csv' con tu propio archivo de datos)
df = pd.read_csv('/content/creditcard.csv')

# Eliminar filas con valores faltantes
df.dropna(inplace=True)

X=df.iloc[:, 1:30]
y=df.iloc[:,30]



# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA con estimación de covarianza robusta
pca = PCA(n_components=2, svd_solver='randomized', whiten=True)
pca.fit(X_scaled)
X_pca = pca.transform(X_scaled)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

from sklearn import svm
#LINEAL
modeloSVM=svm.SVC(kernel='linear',C=1, gamma=0)
modeloSVM.fit(X_train,y_train)
preds_svm = modeloSVM.predict(X_test)
preds_svm
float(np.sum(preds_svm == y_test))/y_test.shape[0]

"""Validación crizada para SVM"""

# Para validar
from sklearn import svm
from sklearn.model_selection import cross_val_score
scores = cross_val_score(modeloSVM, X, y, cv=5)
# Imprimir los puntajes de validación cruzada
print("Puntajes de validación cruzada:", scores)
# Calcular y mostrar el promedio de los puntajes de validación cruzada
print("Puntaje promedio de validación cruzada:", scores.mean())

# Escalar los datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

!pip install scikit-learn
!pip show sklearn.model_selection

import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# Train an SVM classifier with RBF kernel
# svm = SVC(kernel='rbf', random_state=42)

# Learning curve
train_sizes, train_scores, test_scores = learning_curve(modeloSVM, X_train_scaled, y_train, cv=5)
train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_scores_mean, label='Training score')
plt.plot(train_sizes, test_scores_mean, label='Cross-validation score')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.title('Learning Curve')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.svm import SVC
svm = SVC()
svm.fit(X_train_scaled, y_train)

plt.figure(figsize=(8, 6))
plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap='viridis')
plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=100,
            linewidth=1, facecolors='none', edgecolors='k', label='Support Vectors')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Margin')
plt.legend()
plt.grid(True)
plt.show()
# Gráfico de error en función del parámetro C
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
grid_search = GridSearchCV(svm, param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)
scores = grid_search.cv_results_['mean_test_score']

plt.figure(figsize=(8, 6))
plt.plot(param_grid['C'], scores, marker='o', linestyle='-', color='b')
plt.xscale('log')
plt.xlabel('C')
plt.ylabel('Mean Test Score')
plt.title('Error vs C')
plt.grid(True)
plt.show()

!pip install scikit-learn
from sklearn.model_selection import GridSearchCV
# Gráfico de tiempo de ejecución en función del parámetro gamma
param_grid = {'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}
grid_search = GridSearchCV(modeloSVM, param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)
times = grid_search.cv_results_['mean_fit_time']

plt.figure(figsize=(8, 6))
plt.plot(param_grid['gamma'], times, marker='o', linestyle='-', color='b')
plt.xscale('log')
plt.xlabel('Gamma')
plt.ylabel('Mean Fit Time (s)')
plt.title('Fit Time vs Gamma')
plt.grid(True)
plt.show()

"""Modelo híbrido comprendido de los modelos: Regresión logística (RL), Gradient Boosting (Gradient), árbol de decisión (dectree)."""

# Importación de librerías necesarias
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay
import shap
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Cargar datos y preparar las variables
# Reemplaza 'ruta_a_tu_archivo.csv' con la ruta de tu archivo
df = pd.read_csv("/content/creditcard.csv")
X = df.iloc[:, 1:30]
y = df.iloc[:, 30]
# División de datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 2. Configuración del pipeline
scaler = StandardScaler()
pca = PCA(n_components=2)  # Cambia el número de componentes según lo necesario

# Modelos base para el ensemble
log_reg = LogisticRegression(random_state=42)
gb_clf = GradientBoostingClassifier(random_state=42)
dec_tree = DecisionTreeClassifier(random_state=42)

# Pipeline final
pipeline = Pipeline([
    ('scaler', scaler),
    ('pca', pca),
    ('gb_clf', gb_clf)
])

# Entrenar el modelo
pipeline.fit(X_train, y_train)

# 3. Evaluación del modelo
# Predicciones
y_pred = pipeline.predict(X_test)
y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

# Métricas
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
print("Matriz de confusión:\n", confusion_matrix(y_test, y_pred))

# AUC-ROC
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"AUC-ROC: {roc_auc:.2f}")

# Curva ROC
RocCurveDisplay.from_estimator(pipeline, X_test, y_test)
plt.title("Curva ROC")
plt.show()

# 4. Visualización de la matriz de confusión
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=["Clase 0", "Clase 1"], yticklabels=["Clase 0", "Clase 1"])
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.title("Matriz de confusión")
plt.show()


# 6. Validación cruzada
scores = cross_val_score(pipeline, X, y, cv=5, scoring='roc_auc')
print(f"AUC-ROC promedio en validación cruzada: {np.mean(scores):.2f}")

# Importación de librerías necesarias
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay
import shap
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Cargar datos y preparar las variables
# Reemplaza 'ruta_a_tu_archivo.csv' con la ruta de tu archivo
df = pd.read_csv("/content/creditcard.csv")
X = df.iloc[:, 1:30]
y = df.iloc[:, 30]

# División de datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 2. Configuración del pipeline
scaler = StandardScaler()
pca = PCA(n_components=2)  # Cambia el número de componentes según lo necesario

# Modelos base - Fit them with transformed training data
X_train_transformed = pca.fit_transform(scaler.fit_transform(X_train))

log_reg = LogisticRegression(random_state=42).fit(X_train_transformed, y_train)
dec_tree = DecisionTreeClassifier(random_state=42).fit(X_train_transformed, y_train)
gb_clf = GradientBoostingClassifier(random_state=42).fit(X_train_transformed, y_train)

# Ensamble con VotingClassifier
voting_clf = VotingClassifier(
    estimators=[('lr', log_reg), ('dt', dec_tree), ('gb', gb_clf)],
    voting='soft'
)

# Pipeline final
pipeline = Pipeline([
    ('scaler', scaler),
    ('pca', pca),
    ('voting_clf', voting_clf)
])

# Entrenar el modelo - Reintroduce this line to fit the VotingClassifier
pipeline.fit(X_train, y_train)

# 3. Evaluación del modelo
# Función para evaluar modelos y calcular métricas
def evaluate_model(name, model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    print(f"\n--- {name} ---")
    print("Reporte de clasificación:")
    print(classification_report(y_test, y_pred))
    print("Matriz de confusión:")
    print(confusion_matrix(y_test, y_pred))
    print(f"AUC-ROC: {roc_auc:.2f}\n")

    return roc_auc

# Evaluación individual de modelos
auc_log_reg = evaluate_model("Logistic Regression", log_reg, X_train, X_test, y_train, y_test)
auc_dec_tree = evaluate_model("Decision Tree", dec_tree, X_train, X_test, y_train, y_test)
auc_gb_clf = evaluate_model("Gradient Boosting", gb_clf, X_train, X_test, y_train, y_test)

# Evaluación del modelo híbrido
auc_voting_clf = evaluate_model("Voting Classifier (Hybrid)", pipeline.named_steps['voting_clf'], X_train, X_test, y_train, y_test)

# Comparación de resultados
model_results = pd.DataFrame({
    'Modelo': ["Logistic Regression", "Decision Tree", "Gradient Boosting", "Voting Classifier"],
    'AUC-ROC': [auc_log_reg, auc_dec_tree, auc_gb_clf, auc_voting_clf]
})
print("\nResumen de Resultados:")
print(model_results.sort_values(by='AUC-ROC', ascending=False))

# Curva ROC para el modelo híbrido
# Use the voting classifier directly, not the whole pipeline, to avoid PCA transformation
RocCurveDisplay.from_estimator(pipeline.named_steps['voting_clf'],
                               X_test,  # Pass the original X_test with 29 features
                               y_test)
plt.title("Curva ROC - Voting Classifier")
plt.show()

# 6. Validación cruzada
scores = cross_val_score(pipeline, X, y, cv=5, scoring='roc_auc')
print(f"AUC-ROC promedio en validación cruzada: {np.mean(scores):.2f}")

# Precision-Recall AUC and MCC
precision, recall, _ = precision_recall_curve(y_test, y_prob)
pr_auc = auc(recall, precision)
mcc = matthews_corrcoef(y_test, y_pred)

print(f"Precision-Recall AUC: {pr_auc:.4f}")
print(f"MCC: {mcc:.4f}")

# Interpretability with SHAP
explainer = shap.Explainer(best_model.named_steps['classifier'], X_train_pca)
shap_values = explainer(X_test_pca)

# SHAP summary plot
shap.summary_plot(shap_values, X_test_pca)

# SHAP dependency plot for a specific feature (e.g., first principal component)
shap.dependence_plot(0, shap_values, X_test_pca)

"""Modelo Híbirdo comprendido con  "Random Forest": rf,"Gradient Boosting": gb,"Logistic Regression": lr"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, confusion_matrix, roc_curve, auc, f1_score,
                             precision_score, recall_score, classification_report, precision_recall_curve, matthews_corrcoef)
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import shap
import joblib
import time

# Load data
df = pd.read_csv("/content/creditcard.csv")
df.dropna(inplace=True)

# Data exploration (EDA)
print(df.info())
print(df.describe())

# Class distribution
plt.figure(figsize=(6, 4))
sns.countplot(x=df.iloc[:, -1])
plt.title("Class Distribution")
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap")
plt.show()

# Separate features and target variable
X = df.iloc[:, 1:30]
y = df.iloc[:, 30]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

# Reduce dimensionality using PCA
pca = PCA(n_components=5, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Define individual models
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
lr = LogisticRegression(max_iter=1000, random_state=42)

# Evaluate individual models using cross-validation
models = {
    "Random Forest": rf,
    "Gradient Boosting": gb,
    "Logistic Regression": lr
}

metrics = {}
for name, model in models.items():
    pipeline = ImbPipeline(steps=[
        ('resampling', smote),
        ('scaler', scaler),
        ('pca', pca),
        ('classifier', model)
    ])
    scores = cross_val_score(pipeline, X_train, y_train, cv=2, scoring='f1', n_jobs=-1)
    metrics[name] = np.mean(scores)

# Ensemble Learning: Voting Classifier
from sklearn.ensemble import VotingClassifier # Import VotingClassifier

ensemble = VotingClassifier(
    estimators=[('Random Forest', rf), ('Gradient Boosting', gb), ('Logistic Regression',lr)],
    voting='soft',  # Soft voting
    n_jobs=-1  # Utiliza todos los núcleos disponibles
)
ensemble.fit(X_train, y_train)
model.fit(X_train, y_train)

# Almacenar resultados
model_results = {}

for model_name, model in models.items():
    print(f"Training {model_name}...")
    # Use X_train_resampled and y_train_resampled here
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

# Métricas
    f1 = f1_score(y_test, y_pred)
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    precision, recall, _ = precision_recall_curve(y_test, y_proba)

    model_results[model_name] = {
        "model": model,
        "f1_score": f1,
        "roc_auc": roc_auc,
        "fpr": fpr,
        "tpr": tpr,
        "precision": precision,
        "recall": recall
    }

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # Import ConfusionMatrixDisplay
import matplotlib.pyplot as plt


# Inside your loop, replace plot_confusion_matrix with:
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])
# Adjust display_labels to your class names
disp.plot(cmap='Blues')  # You can customize the colormap
plt.title(f"{model_name} Confusion Matrix")
plt.show()
import pandas as pd
import matplotlib.pyplot as plt
# Convertir el diccionario en un DataFrame
metrics_df = pd.DataFrame(metrics).T

# Crear el gráfico de barras
ax = metrics_df.plot(kind='bar', figsize=(12, 7), rot=0, colormap='viridis')
plt.title('Comparison of Model Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Models', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Agregar los valores en las barras
for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', label_type='edge', fontsize=10)

# Ajustar la leyenda
plt.legend(title='Metrics', fontsize=12, loc='lower right')
plt.tight_layout()
plt.show()

# Define individual models
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
lr = LogisticRegression(max_iter=1000, random_state=42)

# Evaluate individual models using cross-validation
models = {
    "Random Forest": rf,
    "Gradient Boosting": gb,
    "Logistic Regression": lr
}

metrics = {}
for name, model in models.items():
    try:  # Use try-except block
        pipeline = ImbPipeline(steps=[
            ('resampling', smote),
            ('scaler', scaler),
            ('pca', pca),
            ('classifier', model)
        ])
        scores = cross_val_score(pipeline, X_train, y_train, cv=2, scoring='f1', n_jobs=-1)
        metrics[name] = np.mean(scores)
    except Exception as e:
        print(f"Error during cross-validation for {name}: {e}")  # Print any errors
        metrics[name] = np.nan  # Assign NaN if cross-validation fails

# Curva Precision-Recall
plt.subplot(1, 2, 2)

for model_name, model in models.items():  # Iterate through the original models dictionary
    try:  # Use try-except to handle potential errors
        y_prob = model.predict_proba(X_test)[:, 1]
        precision, recall, _ = precision_recall_curve(y_test, y_prob)
        plt.plot(recall, precision, label=f"{model_name}")
    except Exception as e:
        print(f"Error plotting {model_name}: {e}")  # Print error message if a model fails

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()

plt.tight_layout()
plt.show()
# Mostrar métricas en consola
print(metrics_df)

# Matriz de confusión para el modelo Ensemble
cm = confusion_matrix(y_test, ensemble.predict(X_test))
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matriz de Confusión - Ensemble')
plt.xlabel('Predicción')
plt.ylabel('Verdadero')
plt.show()

# Curvas ROC
plt.figure(figsize=(10, 8))
for name, model in models.items():
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Random')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curvas ROC')
plt.legend()
plt.grid(True)
plt.show()

# Curva Precision-Recall
plt.subplot(1, 2, 2)
for model_name, results in model_results.items():
    plt.plot(results["recall"], results["precision"], label=f"{model_name}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()

plt.tight_layout()
plt.show()

# Importancia de características (Random Forest como ejemplo)
rf_model = model_results["Random Forest"]["model"]
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.title("Feature Importances (Random Forest)")
plt.show()

# Precision-Recall AUC and MCC
precision, recall, _ = precision_recall_curve(y_test, y_prob)
pr_auc = auc(recall, precision)
mcc = matthews_corrcoef(y_test, y_pred)

print(f"Precision-Recall AUC: {pr_auc:.4f}")
print(f"MCC: {mcc:.4f}")

"""Modelo Híbirdo comprendido con  "Random Forest": rf,"Gradient Boosting": gb,"Logistic Regression": lr"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, confusion_matrix, roc_curve, auc, f1_score,
                             precision_score, recall_score, classification_report, precision_recall_curve, matthews_corrcoef)
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.ensemble import BalancedRandomForestClassifier
import shap
import joblib
import time

# Load data
df = pd.read_csv("/content/creditcard.csv")
df.dropna(inplace=True)

# Data exploration (EDA)
print(df.info())
print(df.describe())

# Class distribution
plt.figure(figsize=(6, 4))
sns.countplot(x=df.iloc[:, -1])
plt.title("Class Distribution")
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap")
plt.show()

# Separate features and target variable
X = df.iloc[:, 1:30]
y = df.iloc[:, 30]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Manejar el desequilibrio de clases usando SMOTE and ADASYN
from sklearn.model_selection import StratifiedKFold
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE

# Realizar PCA y SMOTE una sola vez
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
# Reduce dimensionality using PCA

pca = PCA(n_components=5, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)

# Define individual models
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
lr = LogisticRegression(max_iter=1000, random_state=42)

# Create a hybrid model using VotingClassifier # This should be defined here
hybrid_model = VotingClassifier(
    estimators=[
        ('rf', rf),
        ('gb', gb),
        ('lr', lr)
    ],
    voting='soft'  # Soft voting allows probabilities to be averaged
)
hybrid_model.fit(X_train, y_train)

# Usar validación cruzada con StratifiedKFold (estratificada)
cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)

# Luego realiza la validación cruzada con estos cambios
models = {
    "Random Forest": rf,
    "Gradient Boosting": gb,
    "Logistic Regression": lr,
    "Hybrid": hybrid_model
}

scoring_metrics = ['accuracy', 'f1', 'precision', 'recall']  # Define the list of metrics

metrics = {}
for name, model in models.items():
    pipeline = ImbPipeline(steps=[
        ('scaler', scaler),
        ('pca', pca),
        ('classifier', model)
    ])

    # Calculate scores for each metric
    model_scores = {}  # Store scores for this model
    for metric in scoring_metrics:
        scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=metric, n_jobs=-1)
        model_scores[metric] = np.mean(scores)  # Store the mean score for the metric

    metrics[name] = model_scores  # Store the scores for this model
import pandas as pd
import matplotlib.pyplot as plt

# Convert the dictionary to a DataFrame with an index
metrics_df = pd.DataFrame.from_dict(metrics, orient='index').T

# Transpose the DataFrame if necessary
metrics_df = metrics_df.T

# Crear el gráfico de barras
ax = metrics_df.plot(kind='bar', figsize=(12, 7), rot=0, colormap='viridis')
plt.title('Comparison of Model Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Models', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Agregar los valores en las barras
for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', label_type='edge', fontsize=10)

# Ajustar la leyenda
plt.legend(title='Metrics', fontsize=12, loc='lower right')
plt.tight_layout()
plt.show()
# Mostrar métricas en consola
print(metrics_df)

# Modelos seleccionados
rf = RandomForestClassifier(random_state=42)
gb = GradientBoostingClassifier(random_state=42)
lr = LogisticRegression(max_iter=1000, random_state=42)

models = {
    "Random Forest": rf,
    "Gradient Boosting": gb,
    "Logistic Regression": lr
}

# GridSearch con menos combinaciones
param_grid_rf = {
    'classifier__n_estimators': [50, 100],  # Limitar a dos opciones
    'classifier__max_depth': [10]
}

pipeline_rf = ImbPipeline(steps=[
    ('resampling', smote),
    ('scaler', scaler),
    ('pca', pca),
    ('classifier', rf)
])

grid_search = GridSearchCV(pipeline_rf, param_grid_rf, cv=2, scoring='f1', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Resultados
print("Best Parameters (Random Forest):", grid_search.best_params_)
print("F1-Score:", grid_search.best_score_)
# Create a DataFrame with the F1-Score obtained during GridSearchCV
pipeline_rf_results = pd.DataFrame({'F1-Score': [grid_search.best_score_]}, index=['Random Forest (Tuned)'])

# Then you can concatenate or merge this with the previous results
all_results = pd.concat([metrics_df, pipeline_rf_results])

# And finally, plot the results
ax = all_results.plot(kind='bar', figsize=(12, 10), rot=0)

# Agregar los valores a las barras
for i in ax.patches:
    ax.annotate(f'{i.get_height():.2f}',  # Formato con dos decimales
                (i.get_x() + i.get_width() / 2, i.get_height()),  # Posición
                textcoords="offset points",  # Desplazamiento de texto
                xytext=(0, 5),  # Desplazamiento hacia arriba
                ha='center', fontsize=12, color='black')

plt.title('Model Performance Comparison (Cross-Validation)')
plt.ylabel('F1-Score')
plt.grid(axis='y')
plt.show()

# Evaluate the hybrid model
best_model = grid_search.best_estimator_
start = time.time()
y_pred = best_model.predict(X_test)
end = time.time()
y_prob = best_model.predict_proba(X_test)[:, 1]

# Final metrics
print("Execution Time:", end - start)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1-Score:", f1_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.show()
# ROC curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], 'k--')
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()
# Precision-Recall AUC and MCC
precision, recall, _ = precision_recall_curve(y_test, y_prob)
pr_auc = auc(recall, precision)
mcc = matthews_corrcoef(y_test, y_pred)

print(f"Precision-Recall AUC: {pr_auc:.4f}")
print(f"MCC: {mcc:.4f}")
# Interpretability with SHAP
# Using SHAP for explaining the hybrid model

# ----> Apply PCA to the test data first <----
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test)  # Scale the test data
pca = PCA(n_components=5, random_state=42)    # Create a PCA object
X_test_pca = pca.fit_transform(X_test_scaled) # Apply PCA to the scaled test data

explainer = shap.Explainer(best_model.named_steps['classifier'], X_train_pca)
shap_values = explainer(X_test_pca)

# SHAP summary plot
shap.summary_plot(shap_values, X_test_pca)

# SHAP dependency plot for a specific feature (e.g., first principal component)
shap.dependence_plot(0, shap_values, X_test_pca)
# Analyzing errors (false negatives)
false_negatives = X_test[(y_test == 1) & (y_pred == 0)]
print(f"Number of false negatives: {len(false_negatives)}")

# Heatmap of correlations among false negatives
if len(false_negatives) > 0:
    plt.figure(figsize=(10, 6))
    sns.heatmap(false_negatives.corr(), cmap='coolwarm', annot=False)
    plt.title("Correlation Heatmap of False Negatives")
    plt.show()

# Save the best hybrid model for production
joblib.dump(best_model, "best_hybrid_model.pkl")
print("Best hybrid model saved as 'best_hybrid_model.pkl'")

"""modelo híbrido comprendido con los modelos: Regresión logística (RL), maquina de vectores de soporte (svc), árbol de decisión (dectree)."""

# Importación de librerías necesarias
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay
import shap
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Cargar datos y preparar las variables
# Reemplaza 'ruta_a_tu_archivo.csv' con la ruta de tu archivo
df = pd.read_csv("/content/creditcard.csv")
X = df.iloc[:, 1:30]
y = df.iloc[:, 30]
# División de datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 2. Configuración del pipeline
scaler = StandardScaler()
pca = PCA(n_components=2)  # Cambia el número de componentes según lo necesario

# Modelos base para el VotingClassifier
log_reg = LogisticRegression(random_state=42)
svc = SVC(probability=True, random_state=42)
dec_tree = DecisionTreeClassifier(random_state=42)

# Ensamble con VotingClassifier
voting_clf = VotingClassifier(
    estimators=[('lr', log_reg), ('svc', svc), ('dt', dec_tree)],
    voting='soft'
)

# Pipeline final
pipeline = Pipeline([
    ('scaler', scaler),
    ('pca', pca),
    ('voting_clf', voting_clf)
])

# Entrenar el modelo
pipeline.fit(X_train, y_train)

# 3. Evaluación del modelo
# Predicciones
y_pred = pipeline.predict(X_test)
y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

# Métricas
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
print("Matriz de confusión:\n", confusion_matrix(y_test, y_pred))

# AUC-ROC
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"AUC-ROC: {roc_auc:.2f}")

# Curva ROC
RocCurveDisplay.from_estimator(pipeline, X_test, y_test)
plt.title("Curva ROC")
plt.show()

# 4. Visualización de la matriz de confusión
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=["Clase 0", "Clase 1"], yticklabels=["Clase 0", "Clase 1"])
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.title("Matriz de confusión")
plt.show()

# 6. Validación cruzada
scores = cross_val_score(pipeline, X, y, cv=5, scoring='roc_auc')
print(f"AUC-ROC promedio en validación cruzada: {np.mean(scores):.2f}")

# Ensemble Learning: Voting Classifier
ensemble = VotingClassifier(
    estimators=[('Random Forest', rf), ('Gradient Boosting', gb), ('Logistic Regression',lr)],
    voting='soft',  # Soft voting
    n_jobs=-1  # Utiliza todos los núcleos disponibles
)
ensemble.fit(X_train, y_train)
# Predicciones
models = {'Random Forest': ensemble.estimators_[0],  # Access the fitted RF from ensemble
          'Gradient Boosting': ensemble.estimators_[1], # Access the fitted GB from ensemble
          'Logistic Regression': ensemble.estimators_[2], # Access the fitted LR from ensemble
          'Ensemble': ensemble}
metrics = {}

for name, model in models.items():
    y_pred = model.predict(X_test)
    metrics[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred)
    }

import pandas as pd
import matplotlib.pyplot as plt

# Aquí asumimos que el diccionario "metrics" ya tiene los resultados calculados
# (usando tu bucle con accuracy_score, f1_score, precision_score y recall_score)

# Convertir el diccionario en un DataFrame
metrics_df = pd.DataFrame(metrics).T

# Crear el gráfico de barras
ax = metrics_df.plot(kind='bar', figsize=(12, 7), rot=0, colormap='viridis')
plt.title('Comparison of Model Performance Metrics', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Models', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Agregar los valores en las barras
for container in ax.containers:
    ax.bar_label(container, fmt='%.2f', label_type='edge', fontsize=10)

# Ajustar la leyenda
plt.legend(title='Metrics', fontsize=12, loc='lower right')
plt.tight_layout()
plt.show()
# Mostrar métricas en consola
print(metrics_df)
# Matriz de confusión para el modelo Ensemble
cm = confusion_matrix(y_test, ensemble.predict(X_test))
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matriz de Confusión - Ensemble')
plt.xlabel('Predicción')
plt.ylabel('Verdadero')
plt.show()
# Curvas ROC
plt.figure(figsize=(10, 8))
for name, model in models.items():
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Random')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curvas ROC')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.neural_network import MLPClassifier # Import MLPClassifier



# Initialize and fit the MLP model (before using it)
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42) # Example configuration
mlp.fit(X_train, y_train)

# Entrenar un modelo interpretable para emular las predicciones del MLP
rf = RandomForestClassifier(n_estimators=50, random_state=42)
rf.fit(X_train, mlp.predict(X_train))

# Usar SHAP para interpretar el modelo Random Forest
explainer = shap.TreeExplainer(rf)

# Define X_test_sample if it's not already defined
# For example, you could take a random sample of X_test:
X_test_sample = X_test.sample(n=100, random_state=42)  # Adjust sample size as needed

shap_values = explainer.shap_values(X_test_sample)
shap.summary_plot(shap_values, X_test_sample)